{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<sample name>_<barcode sequence>_L<lane (0-padded to 3 digits)>_R<read number>_<set number (0-padded to 3 digits>.fastq.gz\n",
    "\n",
    "9615-01_S9_L001_R1_001.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18 August 2016:\n",
    "\n",
    "#### Branch v1_errorcatch\n",
    "\n",
    "<b>First</b>: commit <b></b><br/>\n",
    "\n",
    "- branch off of v1 to start working on step 1 exception catching\n",
    "\n",
    "#### Branch v1\n",
    "\n",
    "<b>First</b>: commit <b></b><br/>\n",
    "\n",
    "finish running script, ensure no errors before moving on to exception catching\n",
    "\n",
    "\n",
    "### 17 August 2016:\n",
    "\n",
    "#### Branch v1\n",
    "\n",
    "<b>Second</b>: commit <b>dda7b63269dada3080a96c79c7e72851b56bb739</b><br/> \n",
    "\n",
    "IN PROCESS OF RUNNING\n",
    "\n",
    "- got rid of excel writing (more memory efficient since we are already saving csv; less data lost in case of error/break; simpler)\n",
    "- shortened time of sql vars\n",
    "- modified run / streamline and adjusted to modified functions and classes\n",
    "- annotations to Counts and run \n",
    "- ran and debugged run (works for selected indexes of nate's data) !!\n",
    "- (just need to adjust filtered file thing - not updating...)\n",
    "\n",
    "\n",
    "<b>First</b>: commit <b>ae1ea2ed921c84b5552686a151d2d45ddd2962ad</b><br/>\n",
    "\n",
    "Checkpoint for Counts edits with the following changes\n",
    "- separate qgm_counts_df construction and filtered_df construction for modularity\n",
    "- \"construct_qgm_counts_df\" is the same as the first half of defunct 'construct_df'\n",
    "- new \"construct_filtered_df\" \n",
    "--- takes qgm_counts_df, counts no. molecs and reads PF, named 'molecs' and 'reads' henceforth, and \n",
    "--- filter data by molec(PF) > 0 and all features non-null\n",
    "--- plus some formatting for user \n",
    "\n",
    "### 16 August 2016:\n",
    "\n",
    "#### Branch v1\n",
    "\n",
    "<b>Second</b>: commit <b>bb5de4525e3752a47528d651e7963a25a34b08fb</b><br/>\n",
    "\n",
    "Checkpoint for finishing Counts (ugh finally) and fixing Index\n",
    "- in Index.count_reads, should be type(counts[key] )==list (not np.array -- don't know why it doesn't work. It's probably fixable but i can't deal with it right now because I need to finish all of this other schtuff)\n",
    "- in Counts, constructed generators to manipulate df as static methods:\n",
    "--- 'parse_qgm_key' from input tuple\n",
    "--- 'calculate_count_minscore' to count total and pf reads, bool(molec_passed) according to minscore\n",
    "--- free memory: drop qscore seqs, and eg return intermediate qgm_df to outer function (instead of as property) \n",
    "--- count no. molecs and reads which PF per qg\n",
    "--- set Counts.qg_df property\n",
    "- added xlsx, csv saving functionalities\n",
    "\n",
    "- in progress: amending 'run' to align with other changes, and streamline/declutter; annotations\n",
    "\n",
    "#### (Next:  catching exceptions!!!!!!) also, maybe format motif function to ease user input requirements (e.g. instead of {ATCG}, convert \".\" in function to clean up user input)\n",
    "\n",
    "<b>First</b>: commit<b> 98b5419dad3090c20330b43bb25575f77fe1121e</b><br/>\n",
    "\n",
    "Checkpoint for finished Index, Counts in progress\n",
    "- finished writing Index and surface debugging / catching\n",
    "- cleaned Counts fn construct_df (with helpers parse_qgm_key and calculate_count_minscore) \n",
    "- in progress: debugging construct_df, re-writing counting qg molecs & reads\n",
    "\n",
    "\n",
    "### 15 August 2016:\n",
    "\n",
    "#### Branch v2\n",
    "First: commit <b>8d64e20ace3c1344c216b29a2eae8d71469afac3</b><br/>\n",
    "\n",
    "- set Index to call open_gz, cleaned Index obj:\n",
    "- Added read_ref option for UX flexibility\n",
    "- Streamlined count_reads logic and flow, calling motif_search\n",
    "- Major modifications to motif search; generalize search and feature extraction for each feature and to minimize downstream conditional statements \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NO QTAG ERRORS ALLOWED\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex\n",
    "import os,sys\n",
    "import gzip\n",
    "import sqlalchemy as sqla\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONSTANTS defined by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXPERIMENT = \"2016-08-04-nates1\"\n",
    "INPUT_DIRECTORIES = [\"../data/nate\"]\n",
    "OUTPUT_DIR = \"../output\"\n",
    "\n",
    "QTAG_CSV = \"../helpers/qtags_var.csv\"\n",
    "\n",
    "GTAG_MOTIF = \"CGA(?P<gtag>[ACTG]{3})C(?P<gtag>[ACTG]{4})AATTCGATGG\"\n",
    "MCOUNT_MOTIF = \"C(?P<mcount>[ACTG]{3})C(?P<mcount>[ACTG]{3})C(?P<mcount>[ACTG]{3})GCGCAACGCG\"\n",
    "\n",
    "FILE_MOTIF = \"(?P<sample>.+)_(?P<sample_barcode>.+)_L(?P<lane>\\d{3})_R(?P<read_number>\\d)_(?P<set_number>\\d{3}).fastq.gz\"\n",
    "READ_REF_DEFAULT = {'q':1, 'g':0, 'm':0}\n",
    "\n",
    "IF_SQLTABLE_EXISTS = 'replace'\n",
    "DEFAULT_DB_NAME = \"counts-%s.db\"%EXPERIMENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# used only to make regex motifs, but\n",
    "# not nested to preserve qtag loading functionality if desired\n",
    "def load_qtags(qtag_csv):\n",
    "    try:\n",
    "        qtagdf = pd.DataFrame.from_csv(qtag_csv).reset_index()\n",
    "        qtagdf.rename(columns={'qtag_seq':'seq', 'qtag_num':'qid'}, inplace=True)\n",
    "        qtagdf.qid = qtagdf.qid.apply(lambda x: \"q%s\"%str(x))\n",
    "        qtagdf.seq = qtagdf.seq.str.upper()\n",
    "        qtagdf.set_index('seq', inplace=True)\n",
    "    # TO DO: CHECK FOR DUPLICATE SEQUENCES OR NAMES\n",
    "    except IOError as e:\n",
    "        print \"Unable to load qtag file, with error:\", e\n",
    "        sys.exit(1)\n",
    "    return qtagdf\n",
    "\n",
    "\n",
    "# construct regex motif dict for read search\n",
    "def make_rexs(qtag_csv):\n",
    "    # load and construct qtag motif as OR list of each qtag seq (named)\n",
    "    qtags = load_qtags(qtag_csv)\n",
    "    qtag_phrases = qtags.apply(lambda x: '(?P<%s>%s)'%(x.qid, x.name) , axis=1)    \n",
    "    qtag_motif = \"|\".join( qtag_phrases.values )\n",
    "    # return compiled motifs for qtag, gtag (barcode), and molec counter, resp.\n",
    "    return {'q':regex.compile(qtag_motif, flags=regex.I),\n",
    "            'g':regex.compile(GTAG_MOTIF, flags=regex.I),\n",
    "            'm':regex.compile(MCOUNT_MOTIF, flags=regex.I)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this looks gross but works for now; make pretty later\n",
    "def get_file_list(root):\n",
    "    fpath_temp_a = []\n",
    "    fil_temp_a = []\n",
    "    # construct list of files and their infodict, as tuples:\n",
    "    # (i.e. <sample>_<sample_barcode>_L<lane>_R<read_number>_<set_number>)\n",
    "    for direct, sub, fil in os.walk(root):\n",
    "        fpaths = np.array( [ \"%s/%s\"%(direct,f)  for f in fil] )\n",
    "        to_append = np.array([regex.search(FILE_MOTIF,f) for f in fil ])\n",
    "        fil_temp_a.append( to_append )\n",
    "        fpath_temp_a.append(fpaths)\n",
    "        \n",
    "    fil_temp_b = np.concatenate(fil_temp_a)\n",
    "    fpath_temp_b = np.concatenate(fpath_temp_a)\n",
    "    fil_temp_c = fil_temp_b[np.nonzero(fil_temp_b)]\n",
    "    fpath_temp_c = fpath_temp_b[np.nonzero(fil_temp_b)]\n",
    "    files = np.array( [(fp, fil.groupdict()) for (fp, fil) in zip(fpath_temp_c, fil_temp_c)] )\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_indexes(root):\n",
    "    files = get_file_list(root)\n",
    "    ### FIX :  files list item fmt:  (fpath, fil.str)\n",
    "    indexes = dict([(f[1]['sample'],[\"\",\"\"]) for f in files])\n",
    "    for fpath, match in files:\n",
    "        if match['sample']!='Undetermined':\n",
    "            # assumes 2 reads (fwd and reverse)\n",
    "            indexes[match['sample']][int(match['read_number'])-1] = fpath\n",
    "    if len(indexes) == 0:\n",
    "        print \"Empty index list. No valid files. Please check your input directory and file naming convention.\"\n",
    "        sys.exit(1)            \n",
    "    # convert idx entry list of files to Index object\n",
    "    for idx, idx_paths in indexes.items():\n",
    "        indexes[idx] = Index(idx, idx_paths)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modified opening .gz file with error/exception catching\n",
    "# 15 aug 2016\n",
    "\n",
    "# with zip(gzip.open(self.file0), gzip.open(self.file1)) as f0, f1:\n",
    "def open_gz(fpath):\n",
    "    try:\n",
    "        f_gen = gzip.open(fpath)\n",
    "        return f_gen\n",
    "    except EnvironmentError as e:\n",
    "        print '%s \"%s\". Please check your file and/or directory paths. Skipping index. [EnvironmentError Errno %d]'%(\n",
    "                e.strerror, e.filename, e.errno)\n",
    "    except TypeError as e:\n",
    "        print \"TypeError: %s. Skipping index.\"%e\n",
    "    except BaseException as e:\n",
    "        print 'Other error: %s. Skipping index.'%e\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sysprint(msg,tab_num=0):\n",
    "    tabs = \"\".join([\"\\t\" for t in range(tab_num)])\n",
    "    sys.stdout.write(\"%s%s\\n\"%(tabs, msg))\n",
    "    sys.stdout.flush()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Updated 15 August 2016 -- need to test all class methods together, \n",
    "but otherwise cleaned\n",
    "\n",
    "- Added read_ref option for UX flexibility\n",
    "- Streamlined count_reads logic and flow, calling motif_search\n",
    "- Major modifications to motif search; generalize search and feature extraction for each feature and to minimize downstream conditional statements \n",
    "\n",
    "'''\n",
    "class Index(object):\n",
    "    \n",
    "    # defining read_ref as instance variable so that\n",
    "    # if user uses multiple read rexs or refs, changing\n",
    "    # var won't affect previously defined objects\n",
    "    \n",
    "    def __init__(self, idx, fpaths, read_ref=READ_REF_DEFAULT):\n",
    "        self.idx = idx\n",
    "        self.file0, self.file1 = fpaths\n",
    "        # read_ref as dict\n",
    "        self.read_ref = read_ref\n",
    "        self.tname = regex.sub('[^0-9a-zA-Z]+',\"\",idx)\n",
    "\n",
    "    # so ugly i'm cringing but should probably not change it\n",
    "    # for this v1 version\n",
    "    def count_reads(self):\n",
    "        counts = {}\n",
    "        # such that line 1 is seq, line 3 is qs\n",
    "        line = 0\n",
    "        entry_len = 4\n",
    "        gz0, gz1 = [open_gz(self.file0), open_gz(self.file1)]\n",
    "        if gz0 and gz1:\n",
    "            chunk = [(),()]\n",
    "            for r0,r1 in zip(gz0, gz1):\n",
    "                if line==1: chunk[0] = (r0,r1)  # sequence\n",
    "                elif line==3: chunk[1] = (r0,r1)  # q scores\n",
    "                if line+1 > entry_len:\n",
    "                    key,qscores = self.motif_search(chunk[0],chunk[1])\n",
    "                    counts.setdefault(key,[])\n",
    "                    counts[key].append(qscores)\n",
    "                    chunk = [(),()]\n",
    "                    line = -1\n",
    "                line += 1\n",
    "                \n",
    "        return counts\n",
    "\n",
    "    def motif_search(self, seqs, qscores, order=['q','g','m']):\n",
    "        keys = ['None' for _ in order] \n",
    "        qs_seqs = \"\"\n",
    "        searches = [(feature, read, regex.search(REXS[feature], seqs[read])) \n",
    "                    for feature, read in self.read_ref.items() ]\n",
    "        \n",
    "        for feature, i in zip( order, range(len(order)) ):\n",
    "            r = self.read_ref[feature]\n",
    "            search = regex.search(REXS[feature], seqs[r])\n",
    "            if search:\n",
    "                match = search.capturesdict()\n",
    "                extracted = filter(lambda x: len(match[x])>0, match)\n",
    "                if len(extracted) == 1:\n",
    "                    k = extracted[0]\n",
    "                    keys[i] = k if feature=='q' else \"\".join(match[k])\n",
    "                    qs_seqs += \"\" if feature=='q' else qscores[r][search.start():search.end()]\n",
    "                else:\n",
    "                    print \"Error: non-unique sequence\"\n",
    "            \n",
    "        return tuple(keys), qs_seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "    NOTE ON QSCORE FORMATS (ref. fn calculate_count_minscore)\n",
    "    Q-scores for Illumina 1.8+ (most recent as of Aug 2016) ranges \n",
    "    from 33 to 73 (Phred+33 system). P, the probability of erroneous base call,\n",
    "    is defined as:  P(erroneous base call) = 10 ^ (Qphred / -10), i.e.\n",
    "    for Illumina 1.8+, P = 10^( (QS-33)/-10 ). \n",
    "\n",
    "    The minimum QS cutoff is set at an error probability  \n",
    "    of 10^-3 (standard for Illumina system).\n",
    "'''\n",
    "\n",
    "\n",
    "class Counts(object):\n",
    "    def __init__(self, idx, directory):\n",
    "        self.idx = idx\n",
    "        self.directory = directory\n",
    "    \n",
    "# STATIC METHOD GENERATORS\n",
    "    \n",
    "    # generator for parsing raw df qgm keys\n",
    "    # row is pd.Series\n",
    "    @staticmethod\n",
    "    def parse_qgm_key(row, order=['q','g','m']):\n",
    "        # parse qgm key\n",
    "        for feature, seq in zip(order, row[0]):\n",
    "            row[feature] = seq\n",
    "        return row\n",
    "\n",
    "    # generator for calculating and counting read minscores (PF)\n",
    "    # row is pd.Series\n",
    "    @staticmethod\n",
    "    def calculate_count_minscore(row):\n",
    "        # define variables as null values\n",
    "        keys = [ 'reads_total','reads_pf','molec_passed' ]\n",
    "        vals = [0,0,False]\n",
    "        q, g, m = row[['q','g','m']]\n",
    "        valid = (q!='None') and (g!='None') and (m!='None')\n",
    "        # if key is valid (i.e. non-null q, g, and m), count\n",
    "        if valid:\n",
    "            try:\n",
    "                min_qscores = np.array([])\n",
    "                # get min qscores for each read_qs in row (i.e. per read)\n",
    "                for read_qs in row[1]:\n",
    "                    read_min_qs = 0 if len(read_qs)==0 else np.min(\n",
    "                                        [ord(s) for s in read_qs])\n",
    "                    min_qscores = np.append(min_qscores,read_min_qs)            \n",
    "                # calculate values for variables\n",
    "                vals = [ len(min_qscores), #reads_total\n",
    "                         len(min_qscores[np.where(min_qscores>=63)]), #reads_pf\n",
    "                         True if max(min_qscores) > 0 else False ] #molec_passed \n",
    "            except Exception as e:\n",
    "                print e\n",
    "                print row\n",
    "                sys.exit(1)\n",
    "        # assign new values to row\n",
    "        for k, v in zip(keys, vals) : row[k] = v\n",
    "        return row\n",
    "    \n",
    "    # counts number of molecular counters and reads that PF per qg\n",
    "    # group is pd.DataFrame\n",
    "    @staticmethod\n",
    "    def count_qg(group):\n",
    "        s = pd.Series()\n",
    "        molecs = group.loc[group.reads_pf>0].molec_passed\n",
    "        s['molecs'], s['reads'] = np.sum(molecs), np.sum(group.reads_pf)\n",
    "        return s\n",
    "\n",
    "# INSTANCE FUNCTIONS\n",
    "    \n",
    "    '''  \"construct_qgm_df\" creates pd.DataFrame from Index.counts_dict, and\n",
    "      1) parses qgm key after calling df;\n",
    "      2) calculates min read qscore for each read; and\n",
    "      3) counts reads PF and drops qscore seqs to save memory.\n",
    "      4) classify qgm as 'pass' or 'fail' QC if at least one read PF\n",
    "      5) returns qgm_df to export to db if desired without saving to memory\n",
    "    '''\n",
    "    def construct_qgm_counts_df(self, counts):\n",
    "        # create df from counts; \n",
    "        # abbr qgmcounts_df to df for easier digestion\n",
    "        df = pd.DataFrame.from_dict(counts.items())\n",
    "        # count and consolidate qg\n",
    "        df = df.apply(self.parse_qgm_key, axis=1)\n",
    "        df = df.apply(self.calculate_count_minscore, axis=1)\n",
    "        keep = ['q','g','m','reads_total','reads_pf','molec_passed']\n",
    "        return df[keep]\n",
    "    \n",
    "    '''\n",
    "      construct final filtered df from qgm_counts_df\n",
    "      1) per qg, counts no. molecs PF, reads PF \n",
    "         (as 'molecs' and 'reads', respectively)\n",
    "      2) classify qg FILTER to eliminate extraneous data, ie.\n",
    "         must satisfy (a) molecs > 0, and (b) all features non-null\n",
    "    '''\n",
    "    def construct_filtered_df(self, qgm_counts_df):\n",
    "        # returns bool for the passing conditions \n",
    "        # ie. (has molecs, and all features present/non-null)\n",
    "        def pass_conditions(x): \n",
    "            cond1 = x.molecs > 0\n",
    "            cond2 = not('None' in x[['q','g','m']].values)\n",
    "            return True if cond1 and cond2 else False\n",
    "        \n",
    "        # again, abbr qg_counts_df to df for easier digestion\n",
    "        df = qgm_counts_df.groupby(['q','g'], as_index=False).apply(self.count_qg)\n",
    "        df.loc[:,'passed'] = df.apply(lambda x: pass_conditions(x), axis=1)\n",
    "        \n",
    "        # exclude QC fails, sort and re-index\n",
    "        df = df.loc[df.passed==True]\n",
    "        df.reset_index(inplace=True)\n",
    "        df.sort_values(by=['molecs'], ascending=False, inplace=True)\n",
    "        # for a polished df (re-index to make idx numbers ascending)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        self.filtered_counts = df\n",
    "        print df.head()\n",
    "        print self.filtered_counts.head()\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run(db_name=DEFAULT_DB_NAME, quiet=False):\n",
    "    stats = {}    \n",
    "    \n",
    "    # define output files\n",
    "    db_name = db_name.split(\".db\")[0]\n",
    "    db_path = 'sqlite:///%s/%s.db'%(OUTPUT_DIR, db_name)\n",
    "    engine = sqla.create_engine(db_path)\n",
    "    \n",
    "    \n",
    "    # generate a new csv file and open\n",
    "    filtered_fpath = '%s/filtered-%s'%(OUTPUT_DIR,EXPERIMENT)\n",
    "    open('%s.csv'%filtered_fpath, 'a').close()\n",
    "    f = open('%s.csv'%filtered_fpath, 'a')\n",
    "    header = True\n",
    "    \n",
    "    # iterate through directories/indexes\n",
    "    for directory in INPUT_DIRECTORIES:\n",
    "        # prep indexes \n",
    "        indexes = init_indexes(directory)\n",
    "        sysprint('\\nStarting directory: %s'%directory)\n",
    "\n",
    "        # run analysis for each index\n",
    "        idx_names = indexes.keys()[:5]\n",
    "        i = 1\n",
    "        while i < len(idx_names):\n",
    "            idx_name, index = idx_names[i], indexes[idx_names[i]]\n",
    "            idx_message = '\\nIndex %d of %d: %s'%(i,len(indexes),idx_name)\n",
    "            sysprint(idx_message)\n",
    "\n",
    "            # actually execute analysis\n",
    "            counts_dict = index.count_reads()\n",
    "            sysprint('...counted')\n",
    "\n",
    "            # init counts object with idx name, directory\n",
    "            counts = Counts(idx_name, directory)\n",
    "            qgm_counts_df = counts.construct_qgm_counts_df(counts_dict)\n",
    "            sysprint('...qgm done')\n",
    "            \n",
    "            # save qgm_counts_df to db\n",
    "            conn = engine.connect()\n",
    "            qgm_counts_df.to_sql(idx_name, conn, if_exists='replace') \n",
    "            conn.close()\n",
    "            sysprint('...saved')\n",
    "\n",
    "            # construct and save filtered_df\n",
    "            counts.construct_filtered_df(qgm_counts_df)\n",
    "            counts.filtered_counts['idx'] = idx_name\n",
    "            counts.filtered_counts['directory'] = directory\n",
    "            sysprint('...filtered')\n",
    "\n",
    "            # write to output files for ref\n",
    "            counts.filtered_counts.to_csv(f, header=header)\n",
    "            sysprint('...filtered to csv.')\n",
    "            header = False\n",
    "            i+=1\n",
    "            \n",
    "    f.close()\n",
    "    engine.dispose()\n",
    "    sysprint('Job complete\\n')\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting directory: ../data/nate\n",
      "\n",
      "Index 1 of 16: NH002\n",
      "...counted\n",
      "...qgm done\n",
      "...saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vwl698\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:99: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     q        g  molecs  reads passed\n",
      "0  q26  CATCTTC    8524   8732   True\n",
      "1  q26  CGGGTGC    7078   7212   True\n",
      "2  q26  CGGGTAC      21     21   True\n",
      "3  q26  CGGATGC      11     11   True\n",
      "4  q26  CGGGCGC      10     10   True\n",
      "     q        g  molecs  reads passed\n",
      "0  q26  CATCTTC    8524   8732   True\n",
      "1  q26  CGGGTGC    7078   7212   True\n",
      "2  q26  CGGGTAC      21     21   True\n",
      "3  q26  CGGATGC      11     11   True\n",
      "4  q26  CGGGCGC      10     10   True\n",
      "...filtered\n",
      "...filtered to csv.\n",
      "\n",
      "Index 2 of 16: NH001\n",
      "...counted\n",
      "...qgm done\n",
      "...saved\n",
      "     q        g  molecs  reads passed\n",
      "0  q26  CGGGTGC   15605  16341   True\n",
      "1  q26  CGGATGC      48     48   True\n",
      "2  q26  CGGGTAC      45     45   True\n",
      "3  q26  CGGGCGC      13     13   True\n",
      "4  q26  TGGGTGC      11     11   True\n",
      "     q        g  molecs  reads passed\n",
      "0  q26  CGGGTGC   15605  16341   True\n",
      "1  q26  CGGATGC      48     48   True\n",
      "2  q26  CGGGTAC      45     45   True\n",
      "3  q26  CGGGCGC      13     13   True\n",
      "4  q26  TGGGTGC      11     11   True\n",
      "...filtered\n",
      "...filtered to csv.\n",
      "\n",
      "Index 3 of 16: NH007\n",
      "...counted\n",
      "...qgm done\n",
      "...saved\n",
      "     q        g  molecs  reads passed\n",
      "0  q26  CATCTTC    2915   2941   True\n",
      "1  q26  ATGACGG    2710   2727   True\n",
      "2  q26  TGAGATG    2569   2583   True\n",
      "3  q26  CGGGTGC    2460   2477   True\n",
      "4  q26  TTTTAAT    1847   1857   True\n",
      "     q        g  molecs  reads passed\n",
      "0  q26  CATCTTC    2915   2941   True\n",
      "1  q26  ATGACGG    2710   2727   True\n",
      "2  q26  TGAGATG    2569   2583   True\n",
      "3  q26  CGGGTGC    2460   2477   True\n",
      "4  q26  TTTTAAT    1847   1857   True\n",
      "...filtered\n",
      "...filtered to csv.\n",
      "\n",
      "Index 4 of 16: NH006\n",
      "...counted\n",
      "...qgm done\n",
      "...saved\n",
      "     q        g  molecs  reads passed\n",
      "0  q26  ATGACGG    4152   4203   True\n",
      "1  q26  CATCTTC    4076   4130   True\n",
      "2  q26  CGGGTGC    3221   3256   True\n",
      "3  q26  TGCAAGA    2508   2532   True\n",
      "4  q26  TTTTAAT    2045   2056   True\n",
      "     q        g  molecs  reads passed\n",
      "0  q26  ATGACGG    4152   4203   True\n",
      "1  q26  CATCTTC    4076   4130   True\n",
      "2  q26  CGGGTGC    3221   3256   True\n",
      "3  q26  TGCAAGA    2508   2532   True\n",
      "4  q26  TTTTAAT    2045   2056   True\n",
      "...filtered\n",
      "...filtered to csv.\n",
      "Job complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "REXS = make_rexs(QTAG_CSV)\n",
    "st = run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST CELLS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "'''TEST'''\n",
    "test = '9615-01_S9_L001_R1_001.fastq.gz'\n",
    "REXS = make_rexs(QTAG_CSV)\n",
    "directory = INPUT_DIRECTORIES[0]\n",
    "indexes = init_indexes(directory)\n",
    "testi = indexes.values()[1]\n",
    "counts_dict = testi.count_reads()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "'''TEST'''\n",
    "testcount = Counts(testi.idx)\n",
    "qgm = testcount.construct_qg_df(counts_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEED TO DEAL WITH / REWRITE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "        \n",
    "    def get_stats(self):\n",
    "        valid = self.df.loc[(self.df.qtag!='None')&\n",
    "                            (self.df.gtag!='None')&\n",
    "                            (self.df.mcount!='None')]\n",
    "        idxstats = {\n",
    "            'total reads': len(self.df),\n",
    "            'mcounts with qtag, gtag and mcount': len(valid.groupby(['qtag','gtag','mcount'])),\n",
    "            'reads with qtag, gtag and mcount': len(valid),\n",
    "            'reads with only no qtag': self.get_read_counts(self.df, False, True, True),\n",
    "            'reads with only no gtag': self.get_read_counts(self.df, True, False, True),\n",
    "            'reads with only no mcount': self.get_read_counts(self.df, True, True, False),\n",
    "            'reads with only mcount': self.get_read_counts(self.df,False,False,True),\n",
    "            'reads with only barcode': self.get_read_counts(self.df, False,True,False),\n",
    "            'reads with only qtag': self.get_read_counts(self.df, True,False,False),\n",
    "            'reads with no qtag, barcode or mcount': self.get_read_counts(self.df,False,False,False)\n",
    "        }\n",
    "        \n",
    "        return idxstats\n",
    "pd.DataFrame.from_dict(data_stats).T.to_csv(\"%s/%s_stats.csv\"%(OUTPUT_DIR,EXPERIMENT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE RESOURCES / IDEAS FROM THE NETIZENS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "try:\n",
    "    import json\n",
    "except ImportError:\n",
    "    import simplejson as json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filenames = ['file1.txt', 'file2.txt', ...]\n",
    "with open('path/to/output/file', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "# def format_motif(user):\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import xlsxwriter\n",
    "\n",
    "\n",
    "# Create an new Excel file and add a worksheet.\n",
    "workbook = xlsxwriter.Workbook('demo.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "# Widen the first column to make the text clearer.\n",
    "worksheet.set_column('A:A', 20)\n",
    "\n",
    "# Add a bold format to use to highlight cells.\n",
    "bold = workbook.add_format({'bold': True})\n",
    "\n",
    "# Write some simple text.\n",
    "worksheet.write('A1', 'Hello')\n",
    "\n",
    "# Text with formatting.\n",
    "worksheet.write('A2', 'World', bold)\n",
    "\n",
    "# Write some numbers, with row/column notation.\n",
    "worksheet.write(2, 0, 123)\n",
    "worksheet.write(3, 0, 123.456)\n",
    "\n",
    "# Insert an image.\n",
    "worksheet.insert_image('B5', 'logo.png')\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD CODE I MIGHT NEED"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def testrun(db_name=DEFAULT_DB_NAME, quiet=False):\n",
    "    stats = {}    \n",
    "    counts_output = pd.DataFrame()\n",
    "    dfoutput = pd.DataFrame()\n",
    "    # define output files\n",
    "    db_name = db_name.split(\".db\")[0]\n",
    "    db_path = 'sqlite:///%s/%s.db'%(OUTPUT_DIR, db_name)\n",
    "    engine = sqla.create_engine(db_path)\n",
    "    \n",
    "    filtered_fpath = '%s/filtered-%s'%(OUTPUT_DIR,EXPERIMENT)\n",
    "    try:\n",
    "        # iterate through directories/indexes\n",
    "        for directory in INPUT_DIRECTORIES:\n",
    "            # prep indexes \n",
    "            indexes = init_indexes(directory)\n",
    "            sysprint('\\nStarting directory: %s'%directory.split(\"/\")[-1:][0])\n",
    "            # run analysis for each index\n",
    "            idx_names = indexes.keys()[:2]\n",
    "            i = 1\n",
    "            while i < len(idx_names):\n",
    "                idx_name, index = idx_names[i], indexes[idx_names[i]]\n",
    "                idx_message = '\\nIndex %d of %d: %s'%(i,len(indexes),idx_name)\n",
    "                sysprint(idx_message)\n",
    "                \n",
    "                # actually execute analysis\n",
    "                counts_dict = index.count_reads()\n",
    "                sysprint('...counted')\n",
    "                \n",
    "                # init counts object with idx name, directory\n",
    "                counts = Counts(idx_name, directory)\n",
    "                \n",
    "                qgm_counts_df = counts.construct_qgm_counts_df(counts_dict)\n",
    "                dfoutput = qgm_counts_df\n",
    "                \n",
    "                counts.construct_filtered_df(qgm_counts_df)\n",
    "                counts.filtered_counts['idx'] = idx_name\n",
    "                counts.filtered_counts['directory'] = directory\n",
    "                sysprint('...filtered to csv.')\n",
    "                header = False\n",
    "                i+=1\n",
    "    except Exception as e: \n",
    "        print e\n",
    "        \n",
    "        pass\n",
    "    sysprint('Job complete\\n')\n",
    "    return counts_output, dfoutput\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
